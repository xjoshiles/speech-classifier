# ğŸ—£ï¸ Real vs. Synthetic Speech Classifier

This Streamlit application allows users to upload speech audio files and classify them as either human or AI-generated using pre-trained machine learning models. It supports `.wav`, `.flac`, `.m4a`, `.mp3`, and `.ogg` formats and includes visualisations such as waveform, MFCCs, and spectrograms.

---

## ğŸ”§ Installation & Setup

You can run this application using **Conda**, **pip**, or **Docker**.

---

### âœ… Option 1: Using Conda

1. Create and activate a new environment:
    ```bash
    conda create -n speech-classifier python=3.9 -y
    conda activate speech-classifier
    ```

2. Install `pip` and dependencies:
    ```bash
    conda install pip
    pip install -r requirements.txt
    ```

3. (Optional) Ensure `ffmpeg` is installed for `.m4a` file support:
    - With conda:
      ```bash
      conda install -c conda-forge ffmpeg
      ```

4. Run the app:
    ```bash
    streamlit run app.py
    ```

---

### âœ… Option 2: Using pip (Virtualenv or System Python)

1. Create and activate a virtual environment:
    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows: venv\Scripts\activate
    ```

2. Install dependencies:
    ```bash
    pip install -r requirements.txt
    ```

3. Run the app:
    ```bash
    streamlit run app.py
    ```

---

### âœ… Option 3: Using Docker (for isolated reproducibility)

1. Build the Docker image:
    ```bash
    docker build -t speech-classifier .
    ```

2. Run the container:
    ```bash
    docker run -p 8501:8501 speech-classifier
    ```

3. Open [http://localhost:8501](http://localhost:8501) in your browser.

---

## ğŸ§­ How to Use the Interface

### 1. **Select a Model**
Use the dropdown menu on the left sidebar to choose one of the pretrained models

### 2. **Upload an Audio File**
Drag and drop or browse to upload a file.

- **Supported formats**: `.wav`, `.flac`, `.mp3`, `.m4a`, `.ogg`
- **Maximum size**: 200MB

### 3. **View the Results**
After upload, the app will:

- Automatically convert, preprocess, and standardise the audio
- Extract the corresponding features
- Run the selected model on the extracted features
- Display a classification: **Real** or **Synthetic**
- Show a confidence bar with predicted probability

### 4. **Visualise Results**
The following plots are generated:

- **Waveform**: amplitude over time
- **Spectrogram**: time-frequency representation
- **MFCC**: Mel-frequency cepstral coefficients

---

## ğŸ“‚ File Structure

### ğŸ§­ App Components

Files relevant to running the Streamlit classifier (main app).

```
.
â”œâ”€â”€ app.py                # Main Streamlit app
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ data/                 # Some sample files to try
â”œâ”€â”€ models/               # Model classes
â”œâ”€â”€ results/              # Trained models and configs
â”œâ”€â”€ preprocess/           # Audio standardisation and feature extraction code
â””â”€â”€ README.md             # This file
```


### ğŸ› ï¸ Project Pipeline (for research reproducibility)

Scripts and utilities for end-to-end system development: from data preparation to model training and evaluation. Detailed explanations can be found by inspecting each script, though a high-level overview of their purpose has also been included here.

```
.
â”œâ”€â”€ data/           # Raw + processed speech data
â”‚   â”œâ”€â”€ corpora/    # Original directory for the VCTK corpus
â”‚   â”œâ”€â”€ processed/  # Resampled and normalised audio
â”‚   â””â”€â”€ manifests/  # Metadata for train, val, test splits for all features
â”‚
â”œâ”€â”€ config.py       # Shared constants and configuration settings
â”œâ”€â”€ dataset.py      # Dataset loading and preparation for PyTorch training
â”œâ”€â”€ evaluate.py     # Model evaluation: loss, accuracy, etc.
â”œâ”€â”€ grid_search.py  # Hyperparameter tuning across model-feature combinations
â”œâ”€â”€ train.py        # Train the given model (supports attention mask, DAT)
â”‚
â”œâ”€â”€ preprocess/                      # Data processing and synthesis scripts
â”‚   â”œâ”€â”€ extract_features.py          # Extract MFCC, spectrogram, wav2vec embeddings
â”‚   â”œâ”€â”€ generate_split_manifests.py  # Create speaker/system-disjoint train/val/test splits
â”‚   â”œâ”€â”€ generate_unified_manifests.py
â”‚   â”œâ”€â”€ synthesize_fastpitch.py
â”‚   â”œâ”€â”€ synthesize_openvoice.py
â”‚   â”œâ”€â”€ synthesize_vits.py
â”‚   â”œâ”€â”€ synthesize_xtts.py
â”‚   â”œâ”€â”€ synthesize_yourtts.py
â”‚   â”œâ”€â”€ utils.py                              # Helper functions
â”‚   â”œâ”€â”€ vctk_fix_transcript_case.py           # Fixing VCTK corpus metadata
â”‚   â”œâ”€â”€ vctk_fix_transcript_whitespace.py     # Fixing VCTK corpus metadata
â”‚   â”œâ”€â”€ vctk_generate_transcript_ids.py       # For matching samples on spoken content
â”‚   â”œâ”€â”€ vctk_match_vits_speakers.py           # Match VCTK/VITS speakers due to bug
â”‚   â”œâ”€â”€ vctk_resample.py                      # Standardise + rename VCTK corpus
â”‚   â”œâ”€â”€ vctk_synthesize_vits_for_matching.py
â”‚   â””â”€â”€ vctk_update_speaker_info.py           # Fixing VCTK corpus metadata
â”‚
â”œâ”€â”€ visualise/                      # Plotting and visualisation
â”‚   â”œâ”€â”€ lufs_visualisation.py       # Comparing LUFS loudness across domains
â”‚   â”œâ”€â”€ plots.py                    # Loss curves, ROC curves, Confusion Matrices

```

---

## ğŸ“¦ Data and Models

Due to the large size of the full dataset (approximately **50GB**, including all extracted features), only a subset is included in this repository:

- âœ… **Raw Audio Subset**:  
  For reproducibility and testing, **10 speakers** with **5 utterances** per speaker have been included across both real and synthetic domains. These are stored in the `data/processed` directory.

- âœ… **Model Subset**:  
  Rather than include all trained models (~several hundred across feature and architecture combinations), only the **top-performing model** for each feature/model pairing has been retained in the `results/` folder. Note however that since the Fine-Tuned Wav2Vec model was `~360MB` and exhibited poor performance due to poor training, it was also dropped from the model subset.

---

## ğŸ“¦ Requirements

- Python 3.9
- Streamlit â‰¥ 1.36
- PyTorch, torchaudio
- Hugging Face transformers
- Librosa, Scipy, Scikit-learn
- FFmpeg (for `.m4a` support via `pydub`)

---

## ğŸ‘¤ Author

This application was developed as part of a final-year dissertation on **AI-generated speech detection**.

For questions or feedback, please contact ji91@canterbury.ac.uk.

---
## ğŸ”— Project Repository

Source code: [https://github.com/xjoshiles/speech-classifier](https://github.com/xjoshiles/speech-classifier)
